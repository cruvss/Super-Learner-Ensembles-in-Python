{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**\n",
    "\n",
    "### 1.1 Overview of ensemble learning\n",
    "*Ensemble learning* is a machine learning paradigm where multiple models (often called \"weak learners\") are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined, they can approximate much better complex models.<br>\n",
    "The core idea behind the ensemble learning is that, no single model is perfect, but a combination of diverse models can lead to a more balanced and accurate predictions. Some common emsemble learning techniques are:\n",
    "- Bagging (Bootstrap Aggregating)\n",
    "- Boosting\n",
    "- Stacking\n",
    "\n",
    "### 1.2 What is Super Learner Ensemble?\n",
    "The *Super Learner* is a specific type of stacking ensemble learning technique that stacks multiple traditional machine learning algoriths into an emsemble that finds the optimal combination of diverse learning algorithms. This involves selecting many different algorithms that may be appropriate for your regression or classification problem and evaluating their performance on your dataset using a resampling technique, such as k-fold cross-validation.<br>\n",
    "Steps to build a Super Learner model:<br>\n",
    "1. Select a k-fold split of the training dataset.\n",
    "2. Select m base-models or model configurations.\n",
    "3. For each basemodel:\n",
    "    - Evaluate using k-fold cross-validation.\n",
    "    - Store all out-of-fold predictions.\n",
    "    - Fit the model on the full training dataset and store.\n",
    "4. Fit a meta-model on the out-of-fold predictions.\n",
    "5. Evaluate the model on a holdout dataset or use model to make predictions.\n",
    "\n",
    "![image](super_learner.png)<br>\n",
    "[Image source](https://arxiv.org/abs/1803.02323)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What could be the inputs and outputs for the meta-model?\n",
    "- Inputs: Predictions from the base models\n",
    "- Output: Prediction for training dataset\n",
    "\n",
    "For example, if we have 3 base models, then the meta-model will take 3 predictions as input and output the final prediction.<br>\n",
    "If we had 1000 rows in the training dataset and 3 base models, then the meta-model will have 1000 rows and 3 columns as input and 1000 rows as output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can this work for regression and classification problems?\n",
    "Yes, the Super Learner ensemble can be used for both regression and classification problems. The only difference is the choice of the meta-model. For regression problems, the meta-model can be a linear regression model, while for classification problems, the meta-model can be a logistic regression model or any other classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Won't this overfit the training data?\n",
    "The Super Learner ensemble is designed to reduce overfitting by using cross-validation to evaluate the base models and the meta-model. The base models are trained on different subsets of the training data, and the meta-model is trained on the out-of-fold predictions of the base models. This helps to ensure that the ensemble generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we make a prediction?\n",
    "To make a prediction on a new sample (row of data), first, the row of data is provided as input to each base model to generate a prediction from each model.\n",
    "\n",
    "The predictions from the base-models are then concatenated into a vector and provided as input to the meta-model. The meta-model then makes a final prediction for the row of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation of Super Learner Ensemble With scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement the Super learner for both regression and classification problems using scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Super Learner for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from math import sqrt\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use `make_regression()` method to generate 1000 examples with 100 features. <br>\n",
    "We then split the data so that 50% is used for training and 50% is used for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n"
     ]
    }
   ],
   "source": [
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets define different regression models. We will use the following regression models:\n",
    "- Linear Regression\n",
    "- ElasticNet\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- SVR\n",
    "- Extra Trees Regressor\n",
    "- Bagging Regressor\n",
    "- KNeighbors Regressor\n",
    "- AdaBoost Regressor\n",
    "\n",
    "These models will be used as base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LinearRegression())\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use k-fold cross-validation to make out-of-fold predictions that will be used as the dataset to train the meta-model or “super learner.”\n",
    "\n",
    "For this, first split the data into k folds, 10 in this case. Then, for each fold, we will:\n",
    "- Fit each base model on the training dataset.\n",
    "- Make a prediction on the validation dataset.\n",
    "- Store the predictions for each base model.\n",
    "\n",
    "Each out-of-fold prediction will be a column for the meta-model input. We will collect columns from each algorithm for one fold of the data, horizontally stacking the rows. Then for all groups of columns we collect, we will vertically stack these rows into one long dataset with 500 rows and nine columns.\n",
    "\n",
    "The below function `get_out_of_fold_predictions()` will do the stacking of the predictions from the base models. This is the input dataset for the meta-model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call the function to get the models and the function to prepare the meta-model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (500, 9) (500,)\n"
     ]
    }
   ],
   "source": [
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can fit all of the base-models on the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can fit the meta-model on the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LinearRegression()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can evaluate the base-models on the holdout dataset. [holdout dataset is the test dataset] \n",
    "\n",
    "For the evaluation, we will use the Root Mean Squared Error (MSE) as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, yhat)\n",
    "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use super learner to make prediction on the holdout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n",
      "Meta  (500, 9) (500,)\n",
      "LinearRegression: RMSE 0.560\n",
      "ElasticNet: RMSE 62.114\n",
      "SVR: RMSE 177.241\n",
      "DecisionTreeRegressor: RMSE 144.971\n",
      "KNeighborsRegressor: RMSE 150.702\n",
      "AdaBoostRegressor: RMSE 92.956\n",
      "BaggingRegressor: RMSE 100.110\n",
      "RandomForestRegressor: RMSE 98.980\n",
      "ExtraTreesRegressor: RMSE 96.466\n",
      "Super Learner: RMSE 0.561\n"
     ]
    }
   ],
   "source": [
    "# example of a super learner model for regression\n",
    "from math import sqrt\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LinearRegression())\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    "\n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    "\n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LinearRegression()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, yhat)\n",
    "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n",
    "\n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "fit_base_models(X, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate base models\n",
    "evaluate_models(X_val, y_val, models)\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the shape of the prepared dataset is printed. Then the shape of the dataset for meta-model is displayed. \n",
    "\n",
    "Then the base models are trained on the training dataset and the meta-model is trained on the prepared dataset. After that, the performance of each base-model is reported on holdout dataset. Finally, the performance of the super learner is reported on the holdout dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Super Learner for Classification\n",
    "\n",
    "For the classification problem, the inputs to the meta learner can be class labels or class probabilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use `make_blobs()` method from scikit-learn to generate 1000 examples with 100 features and 2 classes. We will split the data so that 50% is used for training and 50% is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n"
     ]
    }
   ],
   "source": [
    "# create the inputs and outputs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, same as regression, we will define different classification models. We will use the following classification models:\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Extra Trees Classifier\n",
    "- Bagging Classifier\n",
    "- KNeighbors Classifier\n",
    "- AdaBoost Classifier\n",
    "- SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will change get_out_of_fold_predictions() function to return predictions for classification problem.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict_proba(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of Linear Regression, we will use Logistic Regression as the meta-model for classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LogisticRegression(solver='liblinear')\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (500, 18) (500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\desktop\\Two_notebook\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 96.600\n",
      "DecisionTreeClassifier: 71.200\n",
      "SVC: 98.000\n",
      "GaussianNB: 98.400\n",
      "KNeighborsClassifier: 92.800\n",
      "AdaBoostClassifier: 91.000\n",
      "BaggingClassifier: 84.400\n",
      "RandomForestClassifier: 85.400\n",
      "ExtraTreesClassifier: 83.000\n",
      "Super Learner: 98.000\n"
     ]
    }
   ],
   "source": [
    "# example of a super learner model for binary classification\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict_proba(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    "\n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    "\n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LogisticRegression(solver='liblinear')\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tacc = accuracy_score(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, acc*100))\n",
    "# here for the evalaution of the models we are using accuracy_score as the metric\n",
    "\n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict_proba(X)\n",
    "\t\tmeta_X.append(yhat)\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "fit_base_models(X, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate base models\n",
    "evaluate_models(X_val, y_val, models)\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "print('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like regression, we will get the shape of the prepared dataset and the dataset for meta-model. Then, we will train the base models on the training dataset and the meta-model on the prepared dataset. After that, we will report the performance of each base-model on the holdout dataset. Finally, we will report the performance of the super learner on the holdout dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Super Learner With ML-Ensemble Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All above process is manual way of implementing Super Learner. But, we can use `mlens` library to implement Super Learner with few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `mlens` library<br>\n",
    "`pip install mlens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `SuperLearner` class from `mlens` library to implement Super Learner. We can use `.add()` method to add base learners and `.add_meta()` method to add meta learner after instantiating the `SuperLearner` class.<br>\n",
    "```\n",
    "#configure model\n",
    "ensemble = SuperLearner(...)\n",
    "#add list of base learners\n",
    "ensemble.add(...)\n",
    "#add meta learner\n",
    "ensemble.add_meta(...)\n",
    "```\n",
    "\n",
    "To configure the Super Learner, we can use the following parameters:\n",
    "- folds: Number of folds to use in k-fold cross-validation.\n",
    "- scorer: The scoring function to use to evaluate the performance of the base models.\n",
    "- shuffle: Whether to shuffle the data before splitting it into folds.\n",
    "- sample_size: The size of the sample to use when fitting the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Super Learner for Regression using mlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from mlens.ensemble import SuperLearner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n",
      "                                  score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  adaboostregressor         104.40     8.92  1.33  0.05  0.08  0.03\n",
      "layer-1  baggingregressor          120.64    10.83  0.48  0.03  0.02  0.03\n",
      "layer-1  decisiontreeregressor     166.98    15.19  0.09  0.02  0.00  0.00\n",
      "layer-1  elasticnet                 71.18     5.80  0.01  0.01  0.00  0.00\n",
      "layer-1  extratreesregressor       116.00    12.53  0.17  0.03  0.01  0.00\n",
      "layer-1  kneighborsregressor       160.36    11.71  0.00  0.00  0.66  0.04\n",
      "layer-1  linearregression            0.56     0.03  0.02  0.01  0.00  0.00\n",
      "layer-1  randomforestregressor     120.27    10.36  0.38  0.04  0.00  0.00\n",
      "layer-1  svr                       176.40    14.03  0.02  0.00  0.01  0.00\n",
      "\n",
      "Super Learner: RMSE 0.574\n"
     ]
    }
   ],
   "source": [
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LinearRegression())\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# cost function for base models\n",
    "def rmse(yreal, yhat):\n",
    "\treturn sqrt(mean_squared_error(yreal, yhat))\n",
    "\n",
    "\n",
    "# create the super learner\n",
    "def get_super_learner(X):\n",
    "\tensemble = SuperLearner(scorer=rmse, folds=10, shuffle=True, sample_size=len(X))\n",
    "\t# add base models\n",
    "\tmodels = get_models()\n",
    "\tensemble.add(models)\n",
    "\t# add the meta model\n",
    "\tensemble.add_meta(LinearRegression())\n",
    "\treturn ensemble\n",
    " \n",
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# create the super learner\n",
    "ensemble = get_super_learner(X)\n",
    "# fit the super learner\n",
    "ensemble.fit(X, y)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# evaluate meta model\n",
    "yhat = ensemble.predict(X_val)\n",
    "print('Super Learner: RMSE %.3f' % (rmse(y_val, yhat)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, first we will see the shape of prepared datset and dataset for meta-model.\n",
    "\n",
    "Next, the performance for the each base model is displayed.\n",
    "\n",
    "Finally, the performance of the super learner is displayed.\n",
    "\n",
    "In above table, score-m, score-s, ft-m, ft-s, pt-m and pt-s are:\n",
    "- score-m: Mean score of the base model.\n",
    "- score-s: Standard deviation of the score of the base model.\n",
    "- ft-m: Mean fit time of the base model.\n",
    "- ft-s: Standard deviation of the fit time of the base model.\n",
    "- pt-m: Mean prediction time of the base model.\n",
    "- pt-s: Standard deviation of the prediction time of the base model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Super Learner for Classification using mlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use mlens library to implement Super Learner for classification problem. The process is same as regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "# example of a super learner using the mlens library\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sachi\\anaconda3\\envs\\super_learner\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  adaboostclassifier           0.90     0.03  1.27  0.06  0.09  0.02\n",
      "layer-1  baggingclassifier            0.81     0.07  0.42  0.07  0.05  0.02\n",
      "layer-1  decisiontreeclassifier       0.70     0.05  0.08  0.02  0.00  0.00\n",
      "layer-1  extratreesclassifier         0.80     0.04  0.11  0.02  0.02  0.01\n",
      "layer-1  gaussiannb                   0.97     0.03  0.03  0.01  0.01  0.00\n",
      "layer-1  kneighborsclassifier         0.94     0.03  0.00  0.00  0.89  0.02\n",
      "layer-1  logisticregression           0.96     0.02  0.01  0.00  0.00  0.00\n",
      "layer-1  randomforestclassifier       0.82     0.04  0.06  0.01  0.00  0.00\n",
      "layer-1  svc                          0.98     0.03  0.10  0.02  0.00  0.00\n",
      "\n",
      "Super Learner: 95.600\n"
     ]
    }
   ],
   "source": [
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# create the super learner\n",
    "def get_super_learner(X):\n",
    "\tensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X))\n",
    "\t# add base models\n",
    "\tmodels = get_models()\n",
    "\tensemble.add(models)\n",
    "\t# add the meta model\n",
    "\tensemble.add_meta(LogisticRegression(solver='lbfgs'))\n",
    "\treturn ensemble\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# create the super learner\n",
    "ensemble = get_super_learner(X)\n",
    "# fit the super learner\n",
    "ensemble.fit(X, y)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# make predictions on hold out set\n",
    "yhat = ensemble.predict(X_val)\n",
    "print('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_learner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
